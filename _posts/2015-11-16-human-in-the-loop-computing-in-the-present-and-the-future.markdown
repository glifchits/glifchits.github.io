---
published: false
title: Human-in-the-loop computing in the present and the future
layout: post
---
Lukas Blewald recently wrote an [opinion piece](http://www.computerworld.com/article/3004013/robotics/why-human-in-the-loop-computing-is-the-future-of-machine-learning.html) on how "human-in-the-loop computing" is the future of machine learning.
He provides several examples of how it works now, but I think it's a valuable theory, and we should consider how human-in-the-loop computing will evolve over time.

With artificial intelligence, the goal is to identify tasks we can teach computers to do, and then use computers to perform those tasks in an automated fashion. Even though "artificial intelligence" is a very lofty phrase, the scope of these tasks are fairly narrow. Often, AI is really just about expressing a problem in a structured way with input variables, and then exploring all permutations of the input variables until we find a solution to our problem. Perhaps you can see why we can easily teach computers to predict the price of a house given its various characteristics, but we can't easily teach a computer to answer "If you stick a pin into a carrot, does it make a hole in the carrot or in the pin?" [1]

Every time we implement artificial intelligence in a task, it no longer becomes artificial intelligence (this is called the [AI effect](https://en.wikipedia.org/wiki/AI_effect)). I think a corollary is that as soon as we implement an AI task, it becomes "automatable". Blewald shows how right now, our best AI systems are used to automate some things, and humans are used to augment AI knowledge. However, we shouldn't aim to have AI and humans work alongside on the same tasks. We are just slowly making more and more tasks automatable.

We should aim to automate because automatable tasks are not value-creating, they are simply about value-extraction. The nut we have yet failed to crack with AI is how to replicate common sense decision-making and inference, and humans are very good at that. So it turns out that until we unlock the key to strong AI, then humans will always be better at these analytical tasks, at drawing inferences and conclusions.

But why is human-in-the-loop computing here to stay? The fact remains that we still have to *train* AI, and the way we train AI is by having humans tell it what the correct answers are. As we slowly teach AI to perform larger and larger tasks, the scope of their tasks continue to expand and encroach on the frontiers of human capability. There's always more we can teach the computer to do, but we have to be there to train it.

[1] this example was stolen from Ernest Davis and Gary Marcus from their [review article in CACM, Sept 2015](http://dl.acm.org/citation.cfm?doid=2817191.2701413).